# Predict-inference-latency-with-kernel
一种通过分析Kernel执行时属性来预测inference latency的方法。


# 7.7更新：
更新了多模型共存时的预测部分，包括2-4个模型共存时的预测文件及结果（MAE）。

### 当前Version 2022.07.07 已更新
